{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNue1PgIrsSH"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "KOpM0w_brxON",
    "outputId": "d4011ce5-7fa2-478e-cfaf-ac262fa75967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim_sum_ext\n",
      "  Downloading https://files.pythonhosted.org/packages/97/bc/b2d0e3a63fc4af8a62b3ffabd1131bbae03fcad376414e1362fdb0018716/gensim_sum_ext-0.1.2.tar.gz\n",
      "Collecting gensim (from gensim_sum_ext)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/54/1d7294672110d5c0565cabc4b99ed952ced9a2dc2ca1d59ad1b34303a6de/gensim-3.8.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K     |████████████████████████████████| 24.7MB 9.7MB/s eta 0:00:01    |█████▊                          | 4.5MB 5.8MB/s eta 0:00:04     |██████████████████▌             | 14.3MB 1.0MB/s eta 0:00:10\n",
      "\u001b[?25hCollecting pycorenlp (from gensim_sum_ext)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/40/e74eb4fc7906d630b73a84c9ae9d824f694bd4c5a1d727b8e18beadff613/pycorenlp-0.3.0.tar.gz\n",
      "Collecting smart-open>=1.8.1 (from gensim->gensim_sum_ext)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 3.8MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from gensim->gensim_sum_ext) (1.17.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from gensim->gensim_sum_ext) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from gensim->gensim_sum_ext) (1.3.1)\n",
      "Requirement already satisfied: requests in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from pycorenlp->gensim_sum_ext) (2.22.0)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim->gensim_sum_ext) (2.49.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim->gensim_sum_ext)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/3c/00bc2a81bb9d9a0d708bb7f63d63fbb92aef63bc459b01bf317431ca7220/boto3-1.12.5-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from requests->pycorenlp->gensim_sum_ext) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from requests->pycorenlp->gensim_sum_ext) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from requests->pycorenlp->gensim_sum_ext) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from requests->pycorenlp->gensim_sum_ext) (1.24.2)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->smart-open>=1.8.1->gensim->gensim_sum_ext)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.16.0,>=1.15.5 (from boto3->smart-open>=1.8.1->gensim->gensim_sum_ext)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/2b/f290e79241a6b07ab955a65d0395128bb300711ef37589a01696de1c5b5f/botocore-1.15.5-py2.py3-none-any.whl (5.9MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9MB 3.4MB/s eta 0:00:01     |█████████████████████████       | 4.6MB 3.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim->gensim_sum_ext)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.5->boto3->smart-open>=1.8.1->gensim->gensim_sum_ext) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/jlartey10/opt/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.5->boto3->smart-open>=1.8.1->gensim->gensim_sum_ext) (0.15.2)\n",
      "Building wheels for collected packages: gensim-sum-ext, pycorenlp, smart-open\n",
      "  Building wheel for gensim-sum-ext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim-sum-ext: filename=gensim_sum_ext-0.1.2-cp37-none-any.whl size=5041 sha256=e8077b09cf8ce3c41e87108a476ac662819b0cee2b07e9f651b546bcdb387823\n",
      "  Stored in directory: /Users/jlartey10/Library/Caches/pip/wheels/c4/f0/95/fc081a3e8131dc104b19d66b8e895b6b8421bc42f71219d8c6\n",
      "  Building wheel for pycorenlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycorenlp: filename=pycorenlp-0.3.0-cp37-none-any.whl size=2143 sha256=9e16f0b09d612b1fd31fa959ffa111b6948f0251bc1af63e7678d227b30a59d3\n",
      "  Stored in directory: /Users/jlartey10/Library/Caches/pip/wheels/fb/e9/2f/767a7b5f2e82d587a36143c04a21839b4b14bebfb89410d2d5\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-1.9.0-cp37-none-any.whl size=73088 sha256=553ba94765d2b2b9c1a68af2bed82cdbd1655b620fa84ff708a986240b844d34\n",
      "  Stored in directory: /Users/jlartey10/Library/Caches/pip/wheels/ab/10/93/5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built gensim-sum-ext pycorenlp smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim, pycorenlp, gensim-sum-ext\n",
      "Successfully installed boto3-1.12.5 botocore-1.15.5 gensim-3.8.1 gensim-sum-ext-0.1.2 jmespath-0.9.4 pycorenlp-0.3.0 s3transfer-0.3.3 smart-open-1.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jlartey10/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/jlartey10/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jlartey10/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim_sum_ext\n",
    "\n",
    "\n",
    "import bs4 as bs # BeautifulSoup \n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from gensim.summarization import summarize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0hYzvxtr8II"
   },
   "source": [
    "1 (0.25 point) Use the web scraping technique with BeautifulSoup as shown in class to get the text data from the specified data location on the Wikipedia webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "JqKwWUvYr4SF",
    "outputId": "8a23ea51-271a-4f9e-a20a-d47ebec0f66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing (nlp) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
      "the history of natural language processing (nlp) generally started in the 1950s, although work can be found from earlier periods.\n",
      "in 1950, alan turing published an article titled \"computing machinery and intelligence\" which proposed what is now called the turing test as a criterion of intelligence[clarification needed].\n",
      "the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem.[2]  however, real progress was much slower, and after the alpac report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.  little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.\n",
      "some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language syste\n"
     ]
    }
   ],
   "source": [
    "def _scrape_webpage(url): \n",
    "  \"\"\"\n",
    "  Use BeautifulSoup to scrape the webpage text contents. \n",
    "  \"\"\"\n",
    "  scraped_textdata = urllib.request.urlopen(url) \n",
    "  textdata = scraped_textdata.read()\n",
    "  parsed_textdata = bs.BeautifulSoup(textdata,'lxml') \n",
    "  paragraphs = parsed_textdata.find_all('p') \n",
    "  formated_text = \"\"\n",
    "  \n",
    "  for para in paragraphs:\n",
    "    formated_text += para.text\n",
    "  return formated_text\n",
    "\n",
    "\n",
    "mytext = _scrape_webpage('https://en.wikipedia.org/wiki/Natural_language_processing').lower()\n",
    "print(mytext[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y040xWr-_9hk"
   },
   "source": [
    "\n",
    "# 2.   Process the text data and must include:\n",
    "\n",
    "\n",
    "*   2.1 Tokenize the words (0.25 point)\n",
    "*   2.2 Remove the stop words, punctuation, and digit numbers. (0.50 point)\n",
    "*   2.3 write a function to lemmatize the words (1 point)\n",
    "*   2.4 Calculate the word distribution using FreqDist (0.25 point)\n",
    "*   2.5 List and plot the top 15 words (0.25 point)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "qNnAzSvj_-UM",
    "outputId": "cdb41229-5676-4761-c880-1eceafff9695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 374 samples and 790 outcomes>\n",
      "[('languag', 26), ('natur', 19), ('system', 18), ('process', 17), ('learn', 17), ('machin', 16), ('model', 11), ('data', 10), ('translat', 10), ('use', 10), ('mani', 10), ('task', 10), ('research', 9), ('statist', 9), ('rule', 9)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a27a59a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemer and lemmatizer function\n",
    "def stemer_lemmatizer(stopwords_tokens):\n",
    "  processed = []\n",
    "  stemmer = PorterStemmer()\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  for x,test_word in enumerate(stopwords_tokens):\n",
    "    word_stem = stemmer.stem(test_word)\n",
    "    word_lemmatise = lemmatizer.lemmatize(word_stem)\n",
    "    processed.append(word_lemmatise)\n",
    "  return processed\n",
    "  \n",
    "#Text proceccing function\n",
    "def process_text(mytext):\n",
    "  tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "  regexp_tokens = tokenizer.tokenize(mytext.lower())\n",
    "  stopwords_tokens = [token for token in regexp_tokens if token not in stopwords.words('english')]\n",
    "  return stopwords_tokens\n",
    "\n",
    "#Call functions\n",
    "processed = process_text(mytext)\n",
    "processed = stemer_lemmatizer(processed)\n",
    "\n",
    "\n",
    "#List and plot the top 15 words\n",
    "freq_dist = nltk.FreqDist(processed) \n",
    "print(freq_dist)\n",
    "k = 15\n",
    "print(freq_dist.most_common(k)) \n",
    "freq_dist.plot(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_WwJ3NeBYOe"
   },
   "source": [
    "# 3 Summarize the text as shown in class:\n",
    "\n",
    "*   3.1 Calculate word frequency using two different methods:\n",
    "\n",
    "  *   3.1.2 TF-IDF with NLTK (NLTK does not have TF-IDF) (2 points)\n",
    "*   3.2 Score the sentences (1 point)\n",
    "*   3.3 Build a summary (based on ratio, sentence or word count, etc.) (1 point)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_az6wRg6DcMQ"
   },
   "source": [
    "#3.1.1 Weighted frequency (Must use FreqDist to get original word frequencies)\n",
    "\n",
    "\n",
    "A weighted mean is different from our regular arithmetic mean. In weighted mean, each word does not contribute equally in our final mean or does not carry the same weight as the others. Sometimes we would like a word to have more weight based on its semantics. In order to find the weighted mean, we need to multiply the count of words by its weight and add up the results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yu0ms271PfEd"
   },
   "outputs": [],
   "source": [
    "#3.1.1 Weighted frequency (Must use FreqDist to get original word frequencies)\n",
    "\n",
    "\n",
    "DF_Weighted_frequency = pd.DataFrame(columns=[\"WORD\", \"W_FREQUENCY\"])\n",
    "\n",
    "for word, frequency in freq_dist.most_common(k):\n",
    "  DF_Weighted_frequency = DF_Weighted_frequency.append({'WORD': word, 'W_FREQUENCY': frequency}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "r_4l-jt1oqRJ",
    "outputId": "2c861020-b849-4ea5-afe2-2c3b7a8ab0a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>W_FREQUENCY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>languag</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>natur</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>system</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>process</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>learn</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>machin</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>model</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>data</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>translat</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>use</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>mani</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>task</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>research</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>statist</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>rule</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WORD W_FREQUENCY\n",
       "0    languag          26\n",
       "1      natur          19\n",
       "2     system          18\n",
       "3    process          17\n",
       "4      learn          17\n",
       "5     machin          16\n",
       "6      model          11\n",
       "7       data          10\n",
       "8   translat          10\n",
       "9        use          10\n",
       "10      mani          10\n",
       "11      task          10\n",
       "12  research           9\n",
       "13   statist           9\n",
       "14      rule           9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.1.1 Weighted frequency\n",
    "DF_Weighted_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOzefiCZcQoX"
   },
   "source": [
    "Sentence Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kcdv9U9oeO9I",
    "outputId": "43ca01d0-eaad-40a8-a4fc-baabbb9a314a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences:  46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['natur',\n",
       "  'languag',\n",
       "  'process',\n",
       "  'nlp',\n",
       "  'subfield',\n",
       "  'linguist',\n",
       "  'comput',\n",
       "  'scienc',\n",
       "  'inform',\n",
       "  'engin',\n",
       "  'artifici',\n",
       "  'intellig',\n",
       "  'concern',\n",
       "  'interact',\n",
       "  'comput',\n",
       "  'human',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'particular',\n",
       "  'program',\n",
       "  'comput',\n",
       "  'process',\n",
       "  'analyz',\n",
       "  'larg',\n",
       "  'amount',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'data'],\n",
       " ['challeng',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'process',\n",
       "  'frequent',\n",
       "  'involv',\n",
       "  'speech',\n",
       "  'recognit',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'understand',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'gener'],\n",
       " ['histori',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'process',\n",
       "  'nlp',\n",
       "  'gener',\n",
       "  'start',\n",
       "  'although',\n",
       "  'work',\n",
       "  'found',\n",
       "  'earlier',\n",
       "  'period'],\n",
       " ['alan',\n",
       "  'ture',\n",
       "  'publish',\n",
       "  'articl',\n",
       "  'titl',\n",
       "  'comput',\n",
       "  'machineri',\n",
       "  'intellig',\n",
       "  'propos',\n",
       "  'call',\n",
       "  'ture',\n",
       "  'test',\n",
       "  'criterion',\n",
       "  'intellig',\n",
       "  'clarif',\n",
       "  'need'],\n",
       " ['georgetown',\n",
       "  'experi',\n",
       "  'involv',\n",
       "  'fulli',\n",
       "  'automat',\n",
       "  'translat',\n",
       "  'sixti',\n",
       "  'russian',\n",
       "  'sentenc',\n",
       "  'english'],\n",
       " ['author',\n",
       "  'claim',\n",
       "  'within',\n",
       "  'three',\n",
       "  'five',\n",
       "  'year',\n",
       "  'machin',\n",
       "  'translat',\n",
       "  'would',\n",
       "  'solv',\n",
       "  'problem'],\n",
       " ['howev',\n",
       "  'real',\n",
       "  'progress',\n",
       "  'much',\n",
       "  'slower',\n",
       "  'alpac',\n",
       "  'report',\n",
       "  'found',\n",
       "  'ten',\n",
       "  'year',\n",
       "  'long',\n",
       "  'research',\n",
       "  'fail',\n",
       "  'fulfil',\n",
       "  'expect',\n",
       "  'fund',\n",
       "  'machin',\n",
       "  'translat',\n",
       "  'dramat',\n",
       "  'reduc'],\n",
       " ['littl',\n",
       "  'research',\n",
       "  'machin',\n",
       "  'translat',\n",
       "  'conduct',\n",
       "  'late',\n",
       "  'first',\n",
       "  'statist',\n",
       "  'machin',\n",
       "  'translat',\n",
       "  'system',\n",
       "  'develop'],\n",
       " ['notabl',\n",
       "  'success',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'process',\n",
       "  'system',\n",
       "  'develop',\n",
       "  'shrdlu',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'system',\n",
       "  'work',\n",
       "  'restrict',\n",
       "  'block',\n",
       "  'world',\n",
       "  'restrict',\n",
       "  'vocabulari',\n",
       "  'eliza',\n",
       "  'simul',\n",
       "  'rogerian',\n",
       "  'psychotherapist',\n",
       "  'written',\n",
       "  'joseph',\n",
       "  'weizenbaum',\n",
       "  'use',\n",
       "  'almost',\n",
       "  'inform',\n",
       "  'human',\n",
       "  'thought',\n",
       "  'emot',\n",
       "  'eliza',\n",
       "  'sometim',\n",
       "  'provid',\n",
       "  'startlingli',\n",
       "  'human',\n",
       "  'like',\n",
       "  'interact'],\n",
       " ['patient',\n",
       "  'exceed',\n",
       "  'small',\n",
       "  'knowledg',\n",
       "  'base',\n",
       "  'eliza',\n",
       "  'might',\n",
       "  'provid',\n",
       "  'gener',\n",
       "  'respons',\n",
       "  'exampl',\n",
       "  'respond',\n",
       "  'head',\n",
       "  'hurt',\n",
       "  'say',\n",
       "  'head',\n",
       "  'hurt']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.2 Score the sentences (1 point)\n",
    "\n",
    "def sent_tokenize_processing(mytext):\n",
    "  processed_sent_list = []\n",
    "  sent_tokens = sent_tokenize(mytext)\n",
    "  for sentence in sent_tokens:\n",
    "    processed_sent = process_text(sentence)\n",
    "    processed_sent = stemer_lemmatizer(processed_sent)\n",
    "    processed_sent_list.append(processed_sent)\n",
    "  return processed_sent_list\n",
    "\n",
    "processed_sent_list = sent_tokenize_processing(mytext)\n",
    "print(\"# of sentences: \", len(processed_sent_list)) \n",
    "processed_sent_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RCvzWAMcMcj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KH3hYfEbixFa"
   },
   "source": [
    "Calculating DF\n",
    "\n",
    "\n",
    "We need to iterate through all the words in all the document(sentences) and store the document id’s for each word.\n",
    "We are going to create a set if the word doesn’t have a set yet else add to the set. This condition is checked by the try block. Here processed is the body of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cg0xLTo3iQ8A"
   },
   "outputs": [],
   "source": [
    "DF = {}\n",
    "for i in range(len(processed_sent_list)):\n",
    "    tokens = processed_sent_list[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Nuw_8AJYIIOO",
    "outputId": "e2678962-a662-40ae-ed7b-ceee3217a760"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natur': {0, 1, 2, 8, 13, 14, 18, 29, 31, 34, 36, 41, 43},\n",
       " 'languag': {0,\n",
       "  1,\n",
       "  2,\n",
       "  8,\n",
       "  13,\n",
       "  14,\n",
       "  16,\n",
       "  18,\n",
       "  19,\n",
       "  22,\n",
       "  29,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  36,\n",
       "  41,\n",
       "  43},\n",
       " 'process': {0, 1, 2, 8, 13, 14, 16, 18, 29, 31, 33, 34, 36, 41, 43},\n",
       " 'nlp': {0, 2, 31},\n",
       " 'subfield': {0},\n",
       " 'linguist': {0, 15, 16},\n",
       " 'comput': {0, 3, 10, 15, 35},\n",
       " 'scienc': {0},\n",
       " 'inform': {0, 8, 10},\n",
       " 'engin': {0},\n",
       " 'artifici': {0, 45},\n",
       " 'intellig': {0, 3, 45},\n",
       " 'concern': {0},\n",
       " 'interact': {0, 8},\n",
       " 'human': {0, 8, 35},\n",
       " 'particular': {0},\n",
       " 'program': {0},\n",
       " 'analyz': {0},\n",
       " 'larg': {0, 35, 37},\n",
       " 'amount': {0, 24, 27, 28},\n",
       " 'data': {0, 10, 18, 20, 24, 26, 27, 28, 37},\n",
       " 'challeng': {1},\n",
       " 'frequent': {1, 43},\n",
       " 'involv': {1, 4},\n",
       " 'speech': {1, 18, 19, 30},\n",
       " 'recognit': {1, 19},\n",
       " 'understand': {1, 10},\n",
       " 'gener': {1, 2, 9, 20, 27, 37},\n",
       " 'histori': {2},\n",
       " 'start': {2, 14},\n",
       " 'although': {2},\n",
       " 'work': {2, 8, 21, 45},\n",
       " 'found': {2, 6},\n",
       " 'earlier': {2},\n",
       " 'period': {2},\n",
       " 'alan': {3},\n",
       " 'ture': {3},\n",
       " 'publish': {3, 45},\n",
       " 'articl': {3},\n",
       " 'titl': {3},\n",
       " 'machineri': {3},\n",
       " 'propos': {3},\n",
       " 'call': {3, 22, 34, 35},\n",
       " 'test': {3},\n",
       " 'criterion': {3},\n",
       " 'clarif': {3},\n",
       " 'need': {3, 32},\n",
       " 'georgetown': {4},\n",
       " 'experi': {4},\n",
       " 'fulli': {4},\n",
       " 'automat': {4, 35},\n",
       " 'translat': {4, 5, 6, 7, 21, 22, 32},\n",
       " 'sixti': {4, 45},\n",
       " 'russian': {4},\n",
       " 'sentenc': {4},\n",
       " 'english': {4},\n",
       " 'author': {5},\n",
       " 'claim': {5},\n",
       " 'within': {5},\n",
       " 'three': {5},\n",
       " 'five': {5},\n",
       " 'year': {5, 6},\n",
       " 'machin': {5, 6, 7, 14, 16, 17, 21, 29, 32, 34, 35, 36, 41},\n",
       " 'would': {5},\n",
       " 'solv': {5, 42},\n",
       " 'problem': {5},\n",
       " 'howev': {6, 14, 18, 23, 28, 39},\n",
       " 'real': {6, 10, 18, 20, 35, 39, 42},\n",
       " 'progress': {6},\n",
       " 'much': {6, 27, 34},\n",
       " 'slower': {6},\n",
       " 'alpac': {6},\n",
       " 'report': {6},\n",
       " 'ten': {6},\n",
       " 'long': {6},\n",
       " 'research': {6, 7, 18, 21, 24, 25, 34, 39, 41},\n",
       " 'fail': {6},\n",
       " 'fulfil': {6},\n",
       " 'expect': {6},\n",
       " 'fund': {6},\n",
       " 'dramat': {6},\n",
       " 'reduc': {6},\n",
       " 'littl': {7},\n",
       " 'conduct': {7},\n",
       " 'late': {7, 14, 34},\n",
       " 'first': {7, 45},\n",
       " 'statist': {7, 18, 19, 21, 31, 32, 34, 35, 39},\n",
       " 'system': {7, 8, 13, 17, 19, 20, 22, 23, 31, 33, 38, 40, 41},\n",
       " 'develop': {7, 8, 21, 23},\n",
       " 'notabl': {8, 21},\n",
       " 'success': {8, 21, 23},\n",
       " 'shrdlu': {8},\n",
       " 'restrict': {8},\n",
       " 'block': {8},\n",
       " 'world': {8, 10, 20, 28, 35, 42},\n",
       " 'vocabulari': {8},\n",
       " 'eliza': {8, 9},\n",
       " 'simul': {8},\n",
       " 'rogerian': {8},\n",
       " 'psychotherapist': {8},\n",
       " 'written': {8, 12, 13, 17},\n",
       " 'joseph': {8},\n",
       " 'weizenbaum': {8},\n",
       " 'use': {8, 17, 18, 26, 28, 30, 32, 35, 38, 42},\n",
       " 'almost': {8},\n",
       " 'thought': {8},\n",
       " 'emot': {8},\n",
       " 'sometim': {8},\n",
       " 'provid': {8, 9},\n",
       " 'startlingli': {8},\n",
       " 'like': {8},\n",
       " 'patient': {9},\n",
       " 'exceed': {9},\n",
       " 'small': {9},\n",
       " 'knowledg': {9},\n",
       " 'base': {9, 13, 18, 31, 32, 39, 41},\n",
       " 'might': {9},\n",
       " 'respons': {9},\n",
       " 'exampl': {9, 11, 19, 29, 35},\n",
       " 'respond': {9},\n",
       " 'head': {9},\n",
       " 'hurt': {9},\n",
       " 'say': {9},\n",
       " 'mani': {10, 12, 19, 21, 29, 33, 36, 40, 41},\n",
       " 'programm': {10},\n",
       " 'began': {10},\n",
       " 'write': {10, 33},\n",
       " 'conceptu': {10},\n",
       " 'ontolog': {10},\n",
       " 'structur': {10},\n",
       " 'margi': {11},\n",
       " 'schank': {11},\n",
       " 'sam': {11},\n",
       " 'cullingford': {11},\n",
       " 'pam': {11},\n",
       " 'wilenski': {11},\n",
       " 'talespin': {11},\n",
       " 'meehan': {11},\n",
       " 'qualm': {11},\n",
       " 'lehnert': {11},\n",
       " 'polit': {11},\n",
       " 'carbonel': {11},\n",
       " 'plot': {11},\n",
       " 'unit': {11},\n",
       " 'time': {12, 28},\n",
       " 'chatterbot': {12},\n",
       " 'includ': {12, 28, 30, 40},\n",
       " 'parri': {12},\n",
       " 'racter': {12},\n",
       " 'jabberwacki': {12},\n",
       " 'complex': {13, 28},\n",
       " 'set': {13, 33, 35, 37},\n",
       " 'hand': {13, 17, 26, 33, 41},\n",
       " 'rule': {13, 17, 33, 35, 38, 41},\n",
       " 'revolut': {14, 34},\n",
       " 'introduct': {14},\n",
       " 'learn': {14, 16, 17, 24, 25, 26, 27, 29, 30, 32, 34, 35, 36, 41},\n",
       " 'algorithm': {14, 17, 25, 26, 28, 36, 37, 38, 41},\n",
       " 'due': {15, 21, 29},\n",
       " 'steadi': {15},\n",
       " 'increas': {15, 30},\n",
       " 'power': {15},\n",
       " 'see': {15},\n",
       " 'moor': {15},\n",
       " 'law': {15, 22},\n",
       " 'gradual': {15},\n",
       " 'lessen': {15},\n",
       " 'domin': {15},\n",
       " 'chomskyan': {15},\n",
       " 'theori': {15},\n",
       " 'e': {15, 30},\n",
       " 'g': {15, 30},\n",
       " 'transform': {16, 32},\n",
       " 'grammar': {16, 33},\n",
       " 'whose': {16},\n",
       " 'theoret': {16},\n",
       " 'underpin': {16},\n",
       " 'discourag': {16},\n",
       " 'sort': {16},\n",
       " 'corpu': {16, 35},\n",
       " 'underli': {16},\n",
       " 'approach': {16, 31, 32},\n",
       " 'earliest': {17, 38},\n",
       " 'decis': {17, 18, 38, 39},\n",
       " 'tree': {17, 38},\n",
       " 'produc': {17, 20, 22, 27, 38, 40, 41},\n",
       " 'hard': {17, 38},\n",
       " 'similar': {17, 38},\n",
       " 'exist': {17, 22},\n",
       " 'part': {18, 29, 30},\n",
       " 'tag': {18, 30},\n",
       " 'introduc': {18},\n",
       " 'hidden': {18},\n",
       " 'markov': {18},\n",
       " 'model': {18, 19, 20, 21, 29, 32, 39, 40},\n",
       " 'increasingli': {18, 25, 39},\n",
       " 'focus': {18, 25, 39},\n",
       " 'make': {18, 28, 39},\n",
       " 'soft': {18, 39},\n",
       " 'probabilist': {18, 39},\n",
       " 'attach': {18, 39},\n",
       " 'valu': {18, 39},\n",
       " 'weight': {18, 39},\n",
       " 'featur': {18, 37, 39},\n",
       " 'input': {18, 20, 27, 37, 39},\n",
       " 'cach': {19},\n",
       " 'upon': {19},\n",
       " 'reli': {19, 30, 34},\n",
       " 'robust': {20},\n",
       " 'given': {20, 27, 44},\n",
       " 'unfamiliar': {20},\n",
       " 'especi': {20, 21},\n",
       " 'contain': {20, 45},\n",
       " 'error': {20},\n",
       " 'common': {20, 38},\n",
       " 'reliabl': {20, 40},\n",
       " 'result': {20, 22, 24, 27, 28, 29, 40},\n",
       " 'integr': {20},\n",
       " 'larger': {20, 40, 42},\n",
       " 'compris': {20},\n",
       " 'multipl': {20},\n",
       " 'subtask': {20, 42},\n",
       " 'earli': {21, 33},\n",
       " 'occur': {21},\n",
       " 'field': {21},\n",
       " 'ibm': {21},\n",
       " 'complic': {21},\n",
       " 'abl': {22},\n",
       " 'take': {22, 37},\n",
       " 'advantag': {22, 40, 41},\n",
       " 'multilingu': {22},\n",
       " 'textual': {22},\n",
       " 'corpus': {22, 23, 35},\n",
       " 'parliament': {22},\n",
       " 'canada': {22},\n",
       " 'european': {22},\n",
       " 'union': {22},\n",
       " 'government': {22},\n",
       " 'proceed': {22},\n",
       " 'offici': {22},\n",
       " 'correspond': {22},\n",
       " 'govern': {22},\n",
       " 'depend': {23, 30},\n",
       " 'specif': {23},\n",
       " 'task': {23, 27, 29, 30, 36, 41, 42, 43},\n",
       " 'implement': {23},\n",
       " 'often': {23, 28},\n",
       " 'continu': {23},\n",
       " 'major': {23},\n",
       " 'limit': {23, 24},\n",
       " 'great': {24},\n",
       " 'deal': {24},\n",
       " 'gone': {24},\n",
       " 'method': {24, 29},\n",
       " 'effect': {24},\n",
       " 'recent': {25},\n",
       " 'unsupervis': {25},\n",
       " 'semi': {25},\n",
       " 'supervis': {25, 27},\n",
       " 'annot': {26, 28, 35},\n",
       " 'desir': {26},\n",
       " 'answer': {26, 30, 40},\n",
       " 'combin': {26},\n",
       " 'non': {26, 28},\n",
       " 'difficult': {27},\n",
       " 'typic': {27, 35},\n",
       " 'le': {27},\n",
       " 'accur': {27},\n",
       " 'enorm': {28},\n",
       " 'avail': {28},\n",
       " 'among': {28},\n",
       " 'thing': {28},\n",
       " 'entir': {28},\n",
       " 'content': {28},\n",
       " 'wide': {28},\n",
       " 'web': {28},\n",
       " 'inferior': {28},\n",
       " 'low': {28},\n",
       " 'enough': {28},\n",
       " 'practic': {28},\n",
       " 'represent': {29},\n",
       " 'deep': {29, 31, 32},\n",
       " 'neural': {29, 31, 32},\n",
       " 'network': {29, 31},\n",
       " 'style': {29},\n",
       " 'becam': {29},\n",
       " 'widespread': {29},\n",
       " 'flurri': {29},\n",
       " 'show': {29},\n",
       " 'techniqu': {29, 30},\n",
       " 'achiev': {29},\n",
       " 'state': {29},\n",
       " 'art': {29},\n",
       " 'par': {29, 30},\n",
       " 'other': {29, 42},\n",
       " 'popular': {30},\n",
       " 'word': {30, 32, 45},\n",
       " 'embed': {30},\n",
       " 'captur': {30},\n",
       " 'semant': {30},\n",
       " 'properti': {30},\n",
       " 'end': {30},\n",
       " 'higher': {30},\n",
       " 'level': {30},\n",
       " 'question': {30},\n",
       " 'instead': {30, 35},\n",
       " 'pipelin': {30},\n",
       " 'separ': {30},\n",
       " 'intermedi': {30, 32},\n",
       " 'area': {31},\n",
       " 'shift': {31},\n",
       " 'entail': {31},\n",
       " 'substanti': {31},\n",
       " 'chang': {31},\n",
       " 'design': {31, 33},\n",
       " 'may': {31},\n",
       " 'view': {31},\n",
       " 'new': {31},\n",
       " 'paradigm': {31, 35},\n",
       " 'distinct': {31},\n",
       " 'instanc': {32},\n",
       " 'term': {32},\n",
       " 'nmt': {32},\n",
       " 'emphas': {32},\n",
       " 'fact': {32},\n",
       " 'directli': {32},\n",
       " 'sequenc': {32},\n",
       " 'obviat': {32},\n",
       " 'step': {32},\n",
       " 'align': {32},\n",
       " 'smt': {32},\n",
       " 'day': {33},\n",
       " 'code': {33},\n",
       " 'devi': {33},\n",
       " 'heurist': {33},\n",
       " 'stem': {33},\n",
       " 'sinc': {34},\n",
       " 'mid': {34},\n",
       " 'heavili': {34},\n",
       " 'infer': {35},\n",
       " 'analysi': {35},\n",
       " 'plural': {35},\n",
       " 'form': {35},\n",
       " 'document': {35},\n",
       " 'possibl': {35, 40},\n",
       " 'differ': {36, 40},\n",
       " 'class': {36},\n",
       " 'appli': {36},\n",
       " 'handwritten': {38},\n",
       " 'express': {40},\n",
       " 'rel': {40},\n",
       " 'certainti': {40},\n",
       " 'rather': {40},\n",
       " 'one': {40},\n",
       " 'compon': {40},\n",
       " 'follow': {41},\n",
       " 'list': {41},\n",
       " 'commonli': {41, 42},\n",
       " 'direct': {42},\n",
       " 'applic': {42},\n",
       " 'serv': {42},\n",
       " 'aid': {42},\n",
       " 'though': {43},\n",
       " 'close': {43},\n",
       " 'intertwin': {43},\n",
       " 'subdivid': {43},\n",
       " 'categori': {43},\n",
       " 'conveni': {43},\n",
       " 'coars': {44},\n",
       " 'divis': {44},\n",
       " 'road': {45},\n",
       " 'market': {45},\n",
       " 'novel': {45},\n",
       " 'million': {45}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sOPNqugZkRRh",
    "outputId": "64c222c2-2b75-4e70-c739-8ec93331877e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is basically then count of unique words in the documnet.\n",
    "\n",
    "len(DF) #will give the unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVNb00s8qS02"
   },
   "source": [
    "DF will have the word as the key and list of doc id’s as the value. but for DF we don’t actually need the list of docs, we just need the count. so we are going to replace the list with its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "y2zo5G4wqU0N",
    "outputId": "8bc5be6a-06dc-4715-e310-ce7afb5daaea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natur': 13,\n",
       " 'languag': 18,\n",
       " 'process': 15,\n",
       " 'nlp': 3,\n",
       " 'subfield': 1,\n",
       " 'linguist': 3,\n",
       " 'comput': 5,\n",
       " 'scienc': 1,\n",
       " 'inform': 3,\n",
       " 'engin': 1,\n",
       " 'artifici': 2,\n",
       " 'intellig': 3,\n",
       " 'concern': 1,\n",
       " 'interact': 2,\n",
       " 'human': 3,\n",
       " 'particular': 1,\n",
       " 'program': 1,\n",
       " 'analyz': 1,\n",
       " 'larg': 3,\n",
       " 'amount': 4,\n",
       " 'data': 9,\n",
       " 'challeng': 1,\n",
       " 'frequent': 2,\n",
       " 'involv': 2,\n",
       " 'speech': 4,\n",
       " 'recognit': 2,\n",
       " 'understand': 2,\n",
       " 'gener': 6,\n",
       " 'histori': 1,\n",
       " 'start': 2,\n",
       " 'although': 1,\n",
       " 'work': 4,\n",
       " 'found': 2,\n",
       " 'earlier': 1,\n",
       " 'period': 1,\n",
       " 'alan': 1,\n",
       " 'ture': 1,\n",
       " 'publish': 2,\n",
       " 'articl': 1,\n",
       " 'titl': 1,\n",
       " 'machineri': 1,\n",
       " 'propos': 1,\n",
       " 'call': 4,\n",
       " 'test': 1,\n",
       " 'criterion': 1,\n",
       " 'clarif': 1,\n",
       " 'need': 2,\n",
       " 'georgetown': 1,\n",
       " 'experi': 1,\n",
       " 'fulli': 1,\n",
       " 'automat': 2,\n",
       " 'translat': 7,\n",
       " 'sixti': 2,\n",
       " 'russian': 1,\n",
       " 'sentenc': 1,\n",
       " 'english': 1,\n",
       " 'author': 1,\n",
       " 'claim': 1,\n",
       " 'within': 1,\n",
       " 'three': 1,\n",
       " 'five': 1,\n",
       " 'year': 2,\n",
       " 'machin': 13,\n",
       " 'would': 1,\n",
       " 'solv': 2,\n",
       " 'problem': 1,\n",
       " 'howev': 6,\n",
       " 'real': 7,\n",
       " 'progress': 1,\n",
       " 'much': 3,\n",
       " 'slower': 1,\n",
       " 'alpac': 1,\n",
       " 'report': 1,\n",
       " 'ten': 1,\n",
       " 'long': 1,\n",
       " 'research': 9,\n",
       " 'fail': 1,\n",
       " 'fulfil': 1,\n",
       " 'expect': 1,\n",
       " 'fund': 1,\n",
       " 'dramat': 1,\n",
       " 'reduc': 1,\n",
       " 'littl': 1,\n",
       " 'conduct': 1,\n",
       " 'late': 3,\n",
       " 'first': 2,\n",
       " 'statist': 9,\n",
       " 'system': 13,\n",
       " 'develop': 4,\n",
       " 'notabl': 2,\n",
       " 'success': 3,\n",
       " 'shrdlu': 1,\n",
       " 'restrict': 1,\n",
       " 'block': 1,\n",
       " 'world': 6,\n",
       " 'vocabulari': 1,\n",
       " 'eliza': 2,\n",
       " 'simul': 1,\n",
       " 'rogerian': 1,\n",
       " 'psychotherapist': 1,\n",
       " 'written': 4,\n",
       " 'joseph': 1,\n",
       " 'weizenbaum': 1,\n",
       " 'use': 10,\n",
       " 'almost': 1,\n",
       " 'thought': 1,\n",
       " 'emot': 1,\n",
       " 'sometim': 1,\n",
       " 'provid': 2,\n",
       " 'startlingli': 1,\n",
       " 'like': 1,\n",
       " 'patient': 1,\n",
       " 'exceed': 1,\n",
       " 'small': 1,\n",
       " 'knowledg': 1,\n",
       " 'base': 7,\n",
       " 'might': 1,\n",
       " 'respons': 1,\n",
       " 'exampl': 5,\n",
       " 'respond': 1,\n",
       " 'head': 1,\n",
       " 'hurt': 1,\n",
       " 'say': 1,\n",
       " 'mani': 9,\n",
       " 'programm': 1,\n",
       " 'began': 1,\n",
       " 'write': 2,\n",
       " 'conceptu': 1,\n",
       " 'ontolog': 1,\n",
       " 'structur': 1,\n",
       " 'margi': 1,\n",
       " 'schank': 1,\n",
       " 'sam': 1,\n",
       " 'cullingford': 1,\n",
       " 'pam': 1,\n",
       " 'wilenski': 1,\n",
       " 'talespin': 1,\n",
       " 'meehan': 1,\n",
       " 'qualm': 1,\n",
       " 'lehnert': 1,\n",
       " 'polit': 1,\n",
       " 'carbonel': 1,\n",
       " 'plot': 1,\n",
       " 'unit': 1,\n",
       " 'time': 2,\n",
       " 'chatterbot': 1,\n",
       " 'includ': 4,\n",
       " 'parri': 1,\n",
       " 'racter': 1,\n",
       " 'jabberwacki': 1,\n",
       " 'complex': 2,\n",
       " 'set': 4,\n",
       " 'hand': 5,\n",
       " 'rule': 6,\n",
       " 'revolut': 2,\n",
       " 'introduct': 1,\n",
       " 'learn': 14,\n",
       " 'algorithm': 9,\n",
       " 'due': 3,\n",
       " 'steadi': 1,\n",
       " 'increas': 2,\n",
       " 'power': 1,\n",
       " 'see': 1,\n",
       " 'moor': 1,\n",
       " 'law': 2,\n",
       " 'gradual': 1,\n",
       " 'lessen': 1,\n",
       " 'domin': 1,\n",
       " 'chomskyan': 1,\n",
       " 'theori': 1,\n",
       " 'e': 2,\n",
       " 'g': 2,\n",
       " 'transform': 2,\n",
       " 'grammar': 2,\n",
       " 'whose': 1,\n",
       " 'theoret': 1,\n",
       " 'underpin': 1,\n",
       " 'discourag': 1,\n",
       " 'sort': 1,\n",
       " 'corpu': 2,\n",
       " 'underli': 1,\n",
       " 'approach': 3,\n",
       " 'earliest': 2,\n",
       " 'decis': 4,\n",
       " 'tree': 2,\n",
       " 'produc': 7,\n",
       " 'hard': 2,\n",
       " 'similar': 2,\n",
       " 'exist': 2,\n",
       " 'part': 3,\n",
       " 'tag': 2,\n",
       " 'introduc': 1,\n",
       " 'hidden': 1,\n",
       " 'markov': 1,\n",
       " 'model': 8,\n",
       " 'increasingli': 3,\n",
       " 'focus': 3,\n",
       " 'make': 3,\n",
       " 'soft': 2,\n",
       " 'probabilist': 2,\n",
       " 'attach': 2,\n",
       " 'valu': 2,\n",
       " 'weight': 2,\n",
       " 'featur': 3,\n",
       " 'input': 5,\n",
       " 'cach': 1,\n",
       " 'upon': 1,\n",
       " 'reli': 3,\n",
       " 'robust': 1,\n",
       " 'given': 3,\n",
       " 'unfamiliar': 1,\n",
       " 'especi': 2,\n",
       " 'contain': 2,\n",
       " 'error': 1,\n",
       " 'common': 2,\n",
       " 'reliabl': 2,\n",
       " 'result': 7,\n",
       " 'integr': 1,\n",
       " 'larger': 3,\n",
       " 'compris': 1,\n",
       " 'multipl': 1,\n",
       " 'subtask': 2,\n",
       " 'earli': 2,\n",
       " 'occur': 1,\n",
       " 'field': 1,\n",
       " 'ibm': 1,\n",
       " 'complic': 1,\n",
       " 'abl': 1,\n",
       " 'take': 2,\n",
       " 'advantag': 3,\n",
       " 'multilingu': 1,\n",
       " 'textual': 1,\n",
       " 'corpus': 3,\n",
       " 'parliament': 1,\n",
       " 'canada': 1,\n",
       " 'european': 1,\n",
       " 'union': 1,\n",
       " 'government': 1,\n",
       " 'proceed': 1,\n",
       " 'offici': 1,\n",
       " 'correspond': 1,\n",
       " 'govern': 1,\n",
       " 'depend': 2,\n",
       " 'specif': 1,\n",
       " 'task': 8,\n",
       " 'implement': 1,\n",
       " 'often': 2,\n",
       " 'continu': 1,\n",
       " 'major': 1,\n",
       " 'limit': 2,\n",
       " 'great': 1,\n",
       " 'deal': 1,\n",
       " 'gone': 1,\n",
       " 'method': 2,\n",
       " 'effect': 1,\n",
       " 'recent': 1,\n",
       " 'unsupervis': 1,\n",
       " 'semi': 1,\n",
       " 'supervis': 2,\n",
       " 'annot': 3,\n",
       " 'desir': 1,\n",
       " 'answer': 3,\n",
       " 'combin': 1,\n",
       " 'non': 2,\n",
       " 'difficult': 1,\n",
       " 'typic': 2,\n",
       " 'le': 1,\n",
       " 'accur': 1,\n",
       " 'enorm': 1,\n",
       " 'avail': 1,\n",
       " 'among': 1,\n",
       " 'thing': 1,\n",
       " 'entir': 1,\n",
       " 'content': 1,\n",
       " 'wide': 1,\n",
       " 'web': 1,\n",
       " 'inferior': 1,\n",
       " 'low': 1,\n",
       " 'enough': 1,\n",
       " 'practic': 1,\n",
       " 'represent': 1,\n",
       " 'deep': 3,\n",
       " 'neural': 3,\n",
       " 'network': 2,\n",
       " 'style': 1,\n",
       " 'becam': 1,\n",
       " 'widespread': 1,\n",
       " 'flurri': 1,\n",
       " 'show': 1,\n",
       " 'techniqu': 2,\n",
       " 'achiev': 1,\n",
       " 'state': 1,\n",
       " 'art': 1,\n",
       " 'par': 2,\n",
       " 'other': 2,\n",
       " 'popular': 1,\n",
       " 'word': 3,\n",
       " 'embed': 1,\n",
       " 'captur': 1,\n",
       " 'semant': 1,\n",
       " 'properti': 1,\n",
       " 'end': 1,\n",
       " 'higher': 1,\n",
       " 'level': 1,\n",
       " 'question': 1,\n",
       " 'instead': 2,\n",
       " 'pipelin': 1,\n",
       " 'separ': 1,\n",
       " 'intermedi': 2,\n",
       " 'area': 1,\n",
       " 'shift': 1,\n",
       " 'entail': 1,\n",
       " 'substanti': 1,\n",
       " 'chang': 1,\n",
       " 'design': 2,\n",
       " 'may': 1,\n",
       " 'view': 1,\n",
       " 'new': 1,\n",
       " 'paradigm': 2,\n",
       " 'distinct': 1,\n",
       " 'instanc': 1,\n",
       " 'term': 1,\n",
       " 'nmt': 1,\n",
       " 'emphas': 1,\n",
       " 'fact': 1,\n",
       " 'directli': 1,\n",
       " 'sequenc': 1,\n",
       " 'obviat': 1,\n",
       " 'step': 1,\n",
       " 'align': 1,\n",
       " 'smt': 1,\n",
       " 'day': 1,\n",
       " 'code': 1,\n",
       " 'devi': 1,\n",
       " 'heurist': 1,\n",
       " 'stem': 1,\n",
       " 'sinc': 1,\n",
       " 'mid': 1,\n",
       " 'heavili': 1,\n",
       " 'infer': 1,\n",
       " 'analysi': 1,\n",
       " 'plural': 1,\n",
       " 'form': 1,\n",
       " 'document': 1,\n",
       " 'possibl': 2,\n",
       " 'differ': 2,\n",
       " 'class': 1,\n",
       " 'appli': 1,\n",
       " 'handwritten': 1,\n",
       " 'express': 1,\n",
       " 'rel': 1,\n",
       " 'certainti': 1,\n",
       " 'rather': 1,\n",
       " 'one': 1,\n",
       " 'compon': 1,\n",
       " 'follow': 1,\n",
       " 'list': 1,\n",
       " 'commonli': 2,\n",
       " 'direct': 1,\n",
       " 'applic': 1,\n",
       " 'serv': 1,\n",
       " 'aid': 1,\n",
       " 'though': 1,\n",
       " 'close': 1,\n",
       " 'intertwin': 1,\n",
       " 'subdivid': 1,\n",
       " 'categori': 1,\n",
       " 'conveni': 1,\n",
       " 'coars': 1,\n",
       " 'divis': 1,\n",
       " 'road': 1,\n",
       " 'market': 1,\n",
       " 'novel': 1,\n",
       " 'million': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in DF:\n",
    "  DF[i] = len(DF[i])\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ItQasfWFu6Xz",
    "outputId": "f6f55ed2-baaf-4737-ac42-82d74b133372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'inform', 'engin', 'artifici', 'intellig', 'concern', 'interact', 'human', 'particular', 'program', 'analyz', 'larg', 'amount', 'data', 'challeng', 'frequent', 'involv', 'speech', 'recognit', 'understand', 'gener', 'histori', 'start', 'although', 'work', 'found', 'earlier', 'period', 'alan', 'ture', 'publish', 'articl', 'titl', 'machineri', 'propos', 'call', 'test', 'criterion', 'clarif', 'need', 'georgetown', 'experi', 'fulli', 'automat', 'translat', 'sixti', 'russian', 'sentenc', 'english', 'author', 'claim', 'within', 'three', 'five', 'year', 'machin', 'would', 'solv', 'problem', 'howev', 'real', 'progress', 'much', 'slower', 'alpac', 'report', 'ten', 'long', 'research', 'fail', 'fulfil', 'expect', 'fund', 'dramat', 'reduc', 'littl', 'conduct', 'late', 'first', 'statist', 'system', 'develop', 'notabl', 'success', 'shrdlu', 'restrict', 'block', 'world', 'vocabulari', 'eliza', 'simul', 'rogerian', 'psychotherapist', 'written', 'joseph', 'weizenbaum', 'use', 'almost', 'thought', 'emot', 'sometim', 'provid', 'startlingli', 'like', 'patient', 'exceed', 'small', 'knowledg', 'base', 'might', 'respons', 'exampl', 'respond', 'head', 'hurt', 'say', 'mani', 'programm', 'began', 'write', 'conceptu', 'ontolog', 'structur', 'margi', 'schank', 'sam', 'cullingford', 'pam', 'wilenski', 'talespin', 'meehan', 'qualm', 'lehnert', 'polit', 'carbonel', 'plot', 'unit', 'time', 'chatterbot', 'includ', 'parri', 'racter', 'jabberwacki', 'complex', 'set', 'hand', 'rule', 'revolut', 'introduct', 'learn', 'algorithm', 'due', 'steadi', 'increas', 'power', 'see', 'moor', 'law', 'gradual', 'lessen', 'domin', 'chomskyan', 'theori', 'e', 'g', 'transform', 'grammar', 'whose', 'theoret', 'underpin', 'discourag', 'sort', 'corpu', 'underli', 'approach', 'earliest', 'decis', 'tree', 'produc', 'hard', 'similar', 'exist', 'part', 'tag', 'introduc', 'hidden', 'markov', 'model', 'increasingli', 'focus', 'make', 'soft', 'probabilist', 'attach', 'valu', 'weight', 'featur', 'input', 'cach', 'upon', 'reli', 'robust', 'given', 'unfamiliar', 'especi', 'contain', 'error', 'common', 'reliabl', 'result', 'integr', 'larger', 'compris', 'multipl', 'subtask', 'earli', 'occur', 'field', 'ibm', 'complic', 'abl', 'take', 'advantag', 'multilingu', 'textual', 'corpus', 'parliament', 'canada', 'european', 'union', 'government', 'proceed', 'offici', 'correspond', 'govern', 'depend', 'specif', 'task', 'implement', 'often', 'continu', 'major', 'limit', 'great', 'deal', 'gone', 'method', 'effect', 'recent', 'unsupervis', 'semi', 'supervis', 'annot', 'desir', 'answer', 'combin', 'non', 'difficult', 'typic', 'le', 'accur', 'enorm', 'avail', 'among', 'thing', 'entir', 'content', 'wide', 'web', 'inferior', 'low', 'enough', 'practic', 'represent', 'deep', 'neural', 'network', 'style', 'becam', 'widespread', 'flurri', 'show', 'techniqu', 'achiev', 'state', 'art', 'par', 'other', 'popular', 'word', 'embed', 'captur', 'semant', 'properti', 'end', 'higher', 'level', 'question', 'instead', 'pipelin', 'separ', 'intermedi', 'area', 'shift', 'entail', 'substanti', 'chang', 'design', 'may', 'view', 'new', 'paradigm', 'distinct', 'instanc', 'term', 'nmt', 'emphas', 'fact', 'directli', 'sequenc', 'obviat', 'step', 'align', 'smt', 'day', 'code', 'devi', 'heurist', 'stem', 'sinc', 'mid', 'heavili', 'infer', 'analysi', 'plural', 'form', 'document', 'possibl', 'differ', 'class', 'appli', 'handwritten', 'express', 'rel', 'certainti', 'rather', 'one', 'compon', 'follow', 'list', 'commonli', 'direct', 'applic', 'serv', 'aid', 'though', 'close', 'intertwin', 'subdivid', 'categori', 'conveni', 'coars', 'divis', 'road', 'market', 'novel', 'million']\n"
     ]
    }
   ],
   "source": [
    "#Gives us the unique words\n",
    "total_vocab = [x for x in DF]\n",
    "print(total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmV3xpwzLOJt"
   },
   "source": [
    "Calculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M8F4cXMRUtZp",
    "outputId": "3af4374a-7376-4990-8c35-d0c6ff75aa1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of sentences (documents)\n",
    "len(processed_sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K6HofPLrvuZE",
    "outputId": "3ccb0a38-9dc2-4596-c349-345c36375f76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating TF_IDF\n",
    "#math formula\n",
    "#tf(t,d) = count of t in d / number of words in d\n",
    "#Document Frequency\n",
    "#df(t) = occurrence of t in documents\n",
    "#Inverse Document Frequency\n",
    "#idf(t) = N/df\n",
    "\n",
    "\n",
    "# Creating an empty Dataframe with column names only\n",
    "DF_TF_IDF = pd.DataFrame(columns=['WORD', 'TF_IDF'])\n",
    "\n",
    "\n",
    "N = len(processed_sent_list)\n",
    "tf_idf = {}\n",
    "thisdict = {}\n",
    "for i in range(N):\n",
    "  tokens = processed_sent_list[i]\n",
    "  counter = collections.Counter(tokens)\n",
    "  len(processed_sent_list[i])\n",
    "  for token in np.unique(tokens):\n",
    "    #This measures the frequency of a word in a document\n",
    "    tf = counter[token]/len(processed_sent_list[i])\n",
    "    df = DF[token]\n",
    "    #IDF is the inverse of the document frequency which measures the informativeness of term t\n",
    "    idf = np.log(N/(df+1)) #As we cannot divide by 0, we smoothen the value by adding 1 to the denominator\n",
    "    #Finally, by taking a multiplicative value of TF and IDF, we get the TF-IDF score\n",
    "    tf_idf[token] = tf*idf\n",
    "\n",
    "    thisdict.update( {token : tf_idf[token]} )\n",
    "    DF_TF_IDF =  DF_TF_IDF.append({'WORD': token, 'TF_IDF': tf_idf[token]}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "pRZbBi5ukLKb",
    "outputId": "a2d99968-28fe-4f17-ad4b-a3f26da30881"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>TF_IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>amount</td>\n",
       "      <td>0.079257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>analyz</td>\n",
       "      <td>0.111982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>artifici</td>\n",
       "      <td>0.097501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>comput</td>\n",
       "      <td>0.218237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>concern</td>\n",
       "      <td>0.111982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>publish</td>\n",
       "      <td>0.420004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>road</td>\n",
       "      <td>0.241192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>sixti</td>\n",
       "      <td>0.210002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>word</td>\n",
       "      <td>0.187873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>work</td>\n",
       "      <td>0.170708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>727 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         WORD    TF_IDF\n",
       "0      amount  0.079257\n",
       "1      analyz  0.111982\n",
       "2    artifici  0.097501\n",
       "3      comput  0.218237\n",
       "4     concern  0.111982\n",
       "..        ...       ...\n",
       "722   publish  0.420004\n",
       "723      road  0.241192\n",
       "724     sixti  0.210002\n",
       "725      word  0.187873\n",
       "726      work  0.170708\n",
       "\n",
       "[727 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jZG1uybnoke"
   },
   "source": [
    "#Calculating TF-IDF\n",
    "\n",
    "**tf(t,d) = count of t in d / number of words in d**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9LHwMincEqM"
   },
   "source": [
    "#3.3 Build a summary (based on ratio, sentence or word count, etc.) (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "vZ2xYNjBc4gT",
    "outputId": "dd84720a-ec7b-44d8-8af9-a24c00822a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this was due to both the steady increase in computational power (see moore's law) and the gradual lessening of the dominance of chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
      "however, part-of-speech tagging introduced the use of hidden markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
      "such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "since the so-called \"statistical revolution\"[11][12] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning.\n"
     ]
    }
   ],
   "source": [
    "#Summarize based on ration\n",
    "print(summarize(mytext, ratio=0.10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "C1l-cXevc6Nx",
    "outputId": "4e4305b3-4668-42e4-ad6a-d3e497916db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "however, part-of-speech tagging introduced the use of hidden markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n"
     ]
    }
   ],
   "source": [
    "#Summarize based on word count\n",
    "print(summarize(mytext, word_count=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ck0TsOddKS2"
   },
   "outputs": [],
   "source": [
    "# 4 (0.25 point) Summarize the same text data using Gensim with TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76ihvvhCuapK"
   },
   "source": [
    "# 4 (0.25 point) Summarize the same text data using Gensim with TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "0mthpPjbdNOt",
    "outputId": "aa3a30a6-b626-457e-fc62-859456e63049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "this was due to both the steady increase in computational power (see moore's law) and the gradual lessening of the dominance of chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
      "however, part-of-speech tagging introduced the use of hidden markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
      "such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "since the so-called \"statistical revolution\"[11][12] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning.\n",
      "\n",
      "Keywords:\n",
      "   Gensim_Text_Rank\n",
      "0          learning\n",
      "1             learn\n",
      "2              data\n",
      "3              real\n",
      "4         languages\n",
      "..              ...\n",
      "77        decisions\n",
      "78           titled\n",
      "79           speech\n",
      "80        annotated\n",
      "81      annotations\n",
      "\n",
      "[82 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "\n",
    "print('Summary:')\n",
    "print(summarize(mytext, ratio=0.10))\n",
    "\n",
    "DF_Gensim = pd.DataFrame(columns=[\"Gensim_Text_Rank\"])\n",
    "DF_Gensim['Gensim_Text_Rank'] = keywords(mytext, split=True)\n",
    "\n",
    "print('\\nKeywords:')\n",
    "print(DF_Gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTn84A_xdv0j"
   },
   "source": [
    "# 5 (1.50 point) Compare 3.1.1 and 3.1.2 methods with Gensim-TextRank. What’s different and why? Could your methods be improved? And How to improve? Please clearly explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BLmDBTi5exs"
   },
   "source": [
    "Gensim is used to summarize text. The gensim summarizer only works on English words currently. An advantage of using gensim is that, we will not have to go through preprocessing the data as we did with word frequency and TD-IDF, because gensim does it's preprocessing within the function. Gensim removes stopwords, stems text on its own. With the split option, we can return either a list of strings or a single string. We can also adjust the amount of text returned by specifying a ratio or a word count parameter. The ratio specifies the fraction of a the sentence to return and the word count specify the maximum amout of words we want in a summary. Gensim does not only summarize the text, but it also supports keyword extraction as in weighted frequency. \n",
    "\n",
    "Term Frequency — Inverse Document Frequency (TF-IDF) is used to quantify the weight of words in a document. This signifies the importance of each word in the documnet. TF-IDF uses vectorization as a tool to translate words to language the computer will understand. With TF-IDF we can perform analysis like clustering, ranking, and identifying relevant documnets. A disadvantage of TF-IDF compared to Gensim is that, we will have to create our own pre-processing function rather than having it automated in a single function. Gensim is a much faster model than TF-IDF and weighted frequency. \n",
    "\n",
    "\n",
    "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "\n",
    "https://rare-technologies.com/text-summarization-with-gensim/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GShrPIfcGYSp"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TvGKq4AhdyOF"
   },
   "outputs": [],
   "source": [
    "# DF_Weighted_frequency #3.1.1\n",
    "# DF_TF_IDF #TF_IDF\n",
    "\n",
    "# DF_Weighted_frequency.sort_values(by=['W_FREQUENCY'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YR3OjCirXo7"
   },
   "outputs": [],
   "source": [
    "# DF_Weighted_frequency #3.1.1\n",
    "# DF_TF_IDF #TF_IDF\n",
    "\n",
    "# DF_TF_IDF.sort_values(by=['TF_IDF'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsFKBkoTwCzp"
   },
   "outputs": [],
   "source": [
    "# DF_Gensim"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AIT590_JosephLartey_Lab1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
